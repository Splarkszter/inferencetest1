version: "3.8"
services:
  llama-server:
    build:
      context: ./llama-server
      dockerfile: Dockerfile
    image: llama-server:local
    container_name: llama-server
    restart: unless-stopped
    volumes:
      - ./models:/models:rw          # optional local dev use
      - coolify_models:/models:rw   # persistent volume (Coolify will create)
    ports:
      - "8080:8080"
    environment:
      # if you set MODEL_URL in Coolify, the container will download it at first start
      - MODEL_PATH=/models/model.gguf
      - MODEL_URL=${MODEL_URL:-}         # optional: full direct URL to GGUF (HuggingFace "raw" url)
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-}  # optional; used when MODEL_URL requires auth
      - THREADS=${THREADS:-}             # optional: set to number of vCPUs (leave empty to auto-detect)
      - CONTEXT=${CONTEXT:-8192}
    healthcheck:
      test: ["CMD", "curl", "-sS", "http://localhost:8080/health"] 
      interval: 10s
      timeout: 5s
      retries: 6

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - openwebui-data:/app/backend/data
    environment:
      - WEBUI_AUTH=False
    depends_on:
      - llama-server

volumes:
  coolify_models:
  openwebui-data:
